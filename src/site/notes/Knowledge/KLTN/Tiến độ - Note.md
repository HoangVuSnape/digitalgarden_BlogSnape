---
{"dg-publish":true,"permalink":"/knowledge/kltn/tien-do-note/","title":"Ti·∫øn ƒë·ªô","pinned":"false"}
---


# C√¥ng vi·ªác
 - [x] Docker compose  
 - [x] T√¨m hi·ªÉu v·ªÅ XAI ()
 - [x] Em ƒëang x√¢y d·ª±ng data ƒë·ªÉ finetune embedding model -> enhance l·∫°i kh·∫£ nƒÉng embedding c·ªßa model. 
 - [x] Shap,  C√°c ph∆∞∆°ng ph√°p h·ªó tr·ª£ b√™n
 - [x] Cleaning data t·ª´ excel - d√πng gemni ƒë·ªÉ li·ªát k√™ nh∆∞ng t·ª´ kh√≥a ƒë·ªÉ n√≥ t·∫°o graph v√† g·∫Øn tr·ªçng s·ªë. B√™n c·∫°nh ƒë√≥ l√† t·∫°o ra quan h·ªá v√† chi·ªÅu n√≥ c√≥ ƒëi 2 chi·ªÅu hay 1 chi·ªÅu. 
 - [x] Cleaning l·∫°i h·ªá th·ªëng. T·ªëi ∆∞u theo ki·ªÉu package. T√°ch ra 
 - [x] ƒê·ªãnh nghƒ© ra c√°c topic tr∆∞·ªõc ƒë·ªÉ vi·∫øt prompt.
 - [ ] X√¢y b·ªô data test c√°c kh√≠a c·∫°nh c·ªßa c√°c tr∆∞·ªùng ragas. 


# Link:
- [Meeting with teacher](Meeting%20with%20teacher.md)

---
# Ti·∫øn ƒë·ªô 
# Tu·∫ßn 1 - 7.4
- T√¨m hi·ªÉu Large Reasoning Model: Kh√°i ni·ªám, y√™u c·∫ßu v√† h∆∞·ªõng ph√°t tri·ªÉn.
- T√¨m hi·ªÉu th√™m v·ªÅ Agent: Nghi√™n c·ª©u MoE, MLoops.
- T√¨m hi·ªÉu c√°ch vi·∫øt ƒë·ªÅ c∆∞∆°ng: X√¢y d·ª±ng l·ªô tr√¨nh ph√°t tri·ªÉn, x√°c ƒë·ªãnh ph·∫°m vi v√† k·∫ø ho·∫°ch th·ª±c hi·ªán
Kh√≥ khƒÉn: 
- M∆° h·ªì v·ªÅ h∆∞·ªõng ƒëi n√™n ƒëang t√¨m hi·ªÉu c√°c h∆∞·ªõng c√≥ th·ªÉ


# Tu·∫ßn 2 - 13.4
- Ti·∫øp theo c√≥ th·ªÉ l√†m ho√†n thi·ªán th√™m data finetune ƒë·ªÉ-> enhance embedding 
- Model Confidence 
- T√¨m hi·ªÉu v·ªÅ GraphRAG, Explain retrieval tr√™n Github 
- C√°c ph∆∞∆°ng ph√°p Explainable AI - NLP m√† ƒë√£ th·ª±c hi·ªán r·ªìi. (C√≥ 1 bu·ªïi meeting youtube m√¨nh ph·∫£i coi ƒë√£). 

---
- T√åm hi·ªÉu v√† ƒëang x√¢y d·ª±ng b·ªô data ƒë·ªÉ embedding model - ƒë·ªÉ tƒÉng kh·∫£ nƒÉng t√¨m ki·∫øm th√¥ng tin
- T√¨m hi·ªÉu Graph Rag(Knowledge graph) - c√≥ demo v·ªÅ ki·∫øn th·ª©c c·ªßa n√≥
- T√¨m hi·ªÉu c∆° b·∫£n v·ªÅ XAI: c√°c ki·∫øn th·ª©c n·ªÅn c·ªßa n√≥ t·ª´ XgBoost, RForest,

Kh√≥ khƒÉn: C√≥ nhi·ªÅu ki·∫øn th·ª©c v√† kƒ© thu·∫≠t trong lƒ©nh v·ª±c n√†y v√† ch∆∞a t√¨m hi·ªÉu h·∫øt v√† n·∫Øm r√µ ƒë∆∞·ª£c 
# Tu·∫ßn 3 - 20.4 
- Ti·∫øp t·ª•c t√¨m hi·ªÉu v·ªÅ XAI: LIME, SHAP,Feature Importance,.. tr∆∞·ªõc ƒë√¢y
- X√¢y d·ª±ng ti·∫øp b·ªô data embedding : ƒê∆∞·ª£c t·∫ßm 60%
- T√¨m ƒë·ªçc paper v√† c√°c survey li√™n quan 

# Tu·∫ßn 4 -  27.4
- T√¨m hi·ªÉu th√™m c√°c b√†i v·ªÅ XAI g·∫ßn ƒë√¢y: c√°ch di·ªÖn ƒë·∫°t hay c√°ch visualize ƒë·ªÉ tƒÉng ƒë·ªô tin c·∫≠y
- T√¨m hi·ªÉu c√°c b√†i paper li√™n quan lƒ©nh v·ª±c XAI v√† RAG
- ƒêang b·∫Øt tay x√¢y d·ª±ng Graph Rag: ƒëang b·ªã l·ªói
- ƒêang t√¨m hi·ªÉu c√°c n·ªÅn t·∫£ng ƒê·ªÉ s·ª≠ d·ª•ng l√†m Knowledge graph. 

# Tu·∫ßn 5
- L√†m v·ªÅ Graph knowldege. Ra ƒë∆∞·ª£c th·ª±c nghi·ªám trong v√≤ng 3 tu·∫ßn t·ªõi. 
- L√†m ƒë∆∞·ª£c khung li√™n qua ƒë·∫øn paper. 


# Tu·∫ßn 6 4.5
- X·ª≠ l√Ω data v√† test v·ªõi c√°c chunking - clean l·∫°i data c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc.
- Ph√¢n t√≠ch s·ªë l∆∞·ª£ng t·ª´ nghƒ© trong data ƒë·ªÉ hi·ªÉu v√† x√¢y d·ª±ng.  
- X√¢y ƒë·ª±ng b·ªô data graph: g·ªìm filename, tags, topics. (60%)


# Tu·∫ßn 7 11.5
## Note
- ƒê√£ g·∫ßn nh∆∞ ho√†n th√†nh b·ªô graph (L·ªói 1 tr∆∞·ªùng ƒë·∫°i h·ªçc)
	- Trong qu√° tr√¨nh l√†m nh·∫≠n th·∫•y 1 s·ªë l·ªói trong qu√° tr√¨nh x√¢y.  
- ƒêang th·ª±c hi·ªán code ƒë·ªÉ th·ª±c hi·ªán h√≥a v·ªÅ graphrag bao g·ªìm: **KnowledgeGraph**, **QueryEngine**, **Visualizer**. 
	- Qu√° tr√¨nh x√¢y g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ s·ªë l∆∞·ª£ng tags, topic nhi·ªÅu qu√° n√™n k hi·ªán th·ªã ƒë√∫ng n·ªôi dung mong mu·ªën
	- V√† sau khi t√¨m hi·ªÉu th√™m l√† c≈©ng d√≠nh v·∫•n ƒë·ªÅ chunking d√†i qu√° ƒë√£ l√†m kh√¥ng kh√°i qu√°t ƒë·ªß t·ªët v·ªÅ graph 
- M√¨nh ƒëang th·∫•y c√°i h·∫°n ch·∫ø l√† c√°i embedding qu√° d√†i l√†m m·∫•y ƒëi kh·∫£ nƒÉng x√¢y graph knowledge. 
	- C√≥ c·∫£m gi√°c kh√¥ng n√™n x√¢y embedding qu√° d√†i m√† ph·∫£i clean l·∫°i data ƒë·ªÉ c·∫Øt ng·∫Øn b·ªõt. 
	- Data c·ªßa HCMUTE ƒëang b·ªã l·ªói nha. c·∫ßn check l·∫°i sau. 
---
Tu·∫ßn t·ªõi
ƒê·∫ßu ti√™n l√† h∆∞·ªõng ƒë·∫øn l√† 
- Ph√¢n c·ª•m l·∫°i c√°c t√πy √Ω c√°c content v√† 
- Sau ƒë√≥ m·ªõi th·ª≠ nghi·ªám l√† ph·∫£i sort l·∫°i c√°c file name ƒë·ªÉ n√≥ group l·∫°i kho·∫£ng c√°ch. 

## Questions
- Th·∫Øc m·∫Øc v·ªÅ c√°ch setup database.
	- ![](/img/user/assets/images/TD1.png)
	- n√≥ l√† nh·ªØng c√¢u nh·ªè
	- Nh∆∞ng v·ªÅ data c·ªßa m√¨nh th√¨ c√≥ ·ªïn kh√¥ng? th·∫ßy c√≥ kinh nghi·ªám kh√¥ng ·∫°
		- ![](/img/user/assets/images/TD2.png)
		- ![](/img/user/assets/images/TD3.png)

Trong qu√° tr√¨nh em t√¨m hi·ªÉu v√† tham kh·∫£o
- **C√°c d·∫°ng chunking n√≥ ng·∫Øn h∆°n v√† g·ªçn h∆°n nhi·ªÅu so v·ªõi d·∫°ng data c≈© c·ªßa em.**
- ![](/img/user/assets/images/TD4.png)

---
Th·∫ßy cho em xin kinh nghi·ªám l√† em n√™n ƒë√°nh topics v√† tags 
- TH√¨ s·ªë l∆∞·ª£ng l·∫•y t·∫ßm bao nhi√™u nhi√™u 
- ![](/img/user/assets/images/TD5.png)

----
# Sai x√≥t
- Trong 1 chunk th√¨ n√≥ s·∫Ω ph·∫£i c√≥ 1 -3  topics th√¥i l√† t·∫ßm 3 ch·ªØ c√°i. V√† m·ªëi quan h·ªá kh√¥ng qu√° d√†i. ƒê·ªÉ th·ªÉ hi·ªán ƒë∆∞·ª£c h√¨nh ·∫£nh:
	- M√¨nh ƒë√£ t·∫≠n d·ª•ng c√°c semantic chunk c·ªßa d·ª± √°n tr∆∞·ªõc ƒë·ªÉ l√†m n√™n khi l√†m ra m·ªõi ng·ªô nh·∫≠n ra v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i. 
	- ![](/img/user/assets/images/TD6.png)

	- ƒê√¢y l√† c√°ch gi·∫£i quy·∫øt:
	  ![](/img/user/assets/images/TD7.png)
	- ![](/img/user/assets/images/TD8.png)
	- ![](/img/user/assets/images/TD9.png)

---



----
# Tu·∫ßn 8.  
- N·∫øu l√† Concept th√¨ n√™n l·∫•y to√†n b·ªô ƒë·ªÉ l√†m ra ƒëo·∫°n ƒë√≥ l·∫•y c·∫£ c√°c th·ª±c th·ªÉ v√† c√°c key c·ªßa n√≥ lu√¥n
	- ·ªû ƒë√¢y code ƒëang quy ƒë·ªãnh l√† 3 ƒë∆∞·ªùng ...
	- V√† n·∫øu nh∆∞ l·∫•y c√°ch n√†y th√¨ n√≥ s·∫Ω hi·ªán th·ªã c√°i ƒë·∫ßu ti√™n ƒë·∫°i di·ªán cho t√™n c·ªßa node ƒëi hay l√† embedding ƒëi. 
	- TH√¨ n·∫øu v·∫≠y th√¨ khi semantic chunking ch·ª© nh∆∞ b√¨nh th∆∞·ªùng n√≥ kh√¥ng ƒë·∫°i di·ªán ƒë∆∞·ª£c v√† n√≥ s·∫Ω qu√° d√†i kh√¥ng t·ªëi ∆∞u. 
	- C√°i n√†y n·∫øu ƒë√∫ng s·∫Ω c·∫ßn chunking l·∫°i v√† ng·∫Øn h∆°n r·∫•t l√† nhi·ªÅu. 

![](/img/user/assets/images/Pasted image 20250531210017.png)

![](/img/user/assets/images/Pasted image 20250602171158.png)
![](/img/user/assets/images/Pasted image 20250602212920.png)

![](/img/user/assets/images/Pasted image 20250602212936.png)

---
T·∫°o ra th·ª≠ project c·ªßa 1 gmail v√† resquest th·ª≠ xem n√≥ nh∆∞ th·∫ø n√†o. 2 c√°i n√≥ c√≥ trung 1 bill kh√¥ng 


# Tu·∫ßn 8 18.5 
- T·∫°o v√† ho√†nh th√†nh data  chunking theo llm ƒë·ªÉ t·ªëi ∆∞u h∆°n 
- T√°ch ra c√°c topic c·ªßa c√°c tr∆∞·ªùng theo lmm 
- Ti·∫øp t·ª•c fix bug v√† t·ªëi ∆∞u l·∫°i code h·ªá th·ªëng c·ªßa d·ª± √°n c≈© (query transformation, router).
- ƒêi·ªÅu ch·ªânh l·∫°i c√°c prompt cho t·ªëi ∆∞u h∆°n 
# Tu·∫ßn 9 25.5 
- Vi·∫øt Code v√† x√¢y d·ª±ng graph rag v√† x·ª≠ l·ªói. 
- T√¨m hi·ªÉu v√† t·ªëi ∆∞u code cho d·ª± √°n, clean l·∫°i code nh∆∞ng v·∫´n ƒëang c√≤n h·∫°n ch·∫ø 
- T√åm hi·ªÉu v·ªÅ MCP cho k·∫øt n·ªëi data ·ªü d∆∞·ªõi v√† docker 

# Tu·∫ßn 10 1.6
Vi·∫øt code v·ªÅ Graph rag bao g·ªìm x·ª≠ l√Ω data chunk, Query engine, Visualize ƒë∆∞·ªùng ƒëi l·∫°i, t√¨m ra ƒë∆∞·ª£c. 
Test v√† t√¨m l·ªói trong code
Th·ª±c nghi·ªám v·ªõi 1 tr∆∞·ªùng BKU v√† test ra c√°c k·∫øt qu·∫£ 
X√¢y d·ª±ng data ƒë·ªÉ l√†m benchmark


---
# TU·∫ßn 9- 8/6

L·∫•y c·ªßa duy ƒë·ªÉ c√≥ g·ª£i √Ω l√†m 
|                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                 |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| - ƒëi·ªÅu ch·ªânh boost ƒëi·ªÉm rerank cho y·∫øu t·ªë th·ªùi gian (m·ªõi > c≈©)  <br>- x√¢y d·ª±ng benchmark cho 2 ph·∫ßn l√† retrieval v√† generation:  <br>+ retrieval: Recall@k, MRR v√† Hit@k  <br>+ generation: ROUGE-L, BERTScore, ChrF v√† Avg Similarity (so v·ªõi eval chu·∫©n nh∆∞ng ch∆∞a c√≥) v√† Avg LLM latency | - g·∫∑p v·∫•n ƒë·ªÅ v·ªõi b·ªô QA ch∆∞a ƒë·ªß ch√≠nh x√°c (do d√πng Gemini), kh√≥ khƒÉn trong vi·ªác t·∫°o n√™n b·ªô benchmark chu·∫©n khi kh√¥ng c√≥ b·ªô evaluation ƒë·ªß t·ªët  <br>- t√¨m hi·ªÉu v√† th·∫•y ƒë∆∞·ª£c t·∫≠p dataset [https://huggingface.co/datasets/thangvip/vilawqa-syllo](https://huggingface.co/datasets/thangvip/vilawqa-syllo) ƒë·∫ßy ƒë·ªß th√¥ng tin, ƒëang trong qu√° tr√¨nh t√¨m hi·ªÉu v√† t√°i s·ª≠ d·ª•ng 1 s·ªë l∆∞·ª£ng nh·ªè d·ªØ li·ªáu (dataset c√≥ 1000 d√≤ng v·ªõi 5 columns: question, syllogysim_answer, domain, refs, reference_texts) | - t√¨m hi·ªÉu t·∫°o t·∫≠p evaluation ƒë·ªÉ t·∫°o c·∫∑p QA cho c√°c c√¢u h·ªèi ph√°p lu·∫≠t  <br>- ti·∫øp t·ª•c ƒëi·ªÅu ch·ªânh prompt t·ªëi ∆∞u  <br>- thi·∫øt l·∫≠p prompt cho model Gemini ƒë·ªÉ t·∫°o c√°c c·∫∑p QA bao g·ªìm ngu·ªìn cho c√¢u tr·∫£ l·ªùi v·ªõi input l√† c√°c vƒÉn b·∫£n ph√°p lu·∫≠t g·ªëc ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch |


T·∫°o ra th√™m data ƒë·ªÉ test ƒë·ªß h∆°n nhi·ªÅu kh√≠a c·∫°nh trong knowledge. 
C·∫ßn l√†m data embedding ƒë·ªÉ l·∫•y ƒë√°nh gi√°. 
TH√¨ neoj4 t√°ch th·ª≠ th·ªÉ v√† benchmark th·∫ø n√†o. C·∫ßn ch·ªâ s·ªë g√¨ ƒë·ªÉ so s√°nh. 

---
C·∫ßn l√† benchmark cho d·ªØ li·ªáu.
Th·ª±c nghi·ªám v·ªõi nhi·ªÅu Embedding model 
- Crawl data 
- VI√™t b√°o c√°o. 


--- 
C·∫ßn ƒëi s√¢u 1 v√¥ m·ªôt model ƒëi s√¢u v√†o data c·ªßa model m√¨nh l·∫•y t·∫ßm 3B  g√¨ ƒë√≥ ƒë·ªÉ c√≥ 
- 1 Case ƒë√∫ng 
- V√† b·ªô test case sai ƒë·ªÉ ra k·∫øt qu·∫£ nh∆∞ th·∫ø n√†o. 

---

NEoj4 -> ....
-  D·∫°ng th·ª±c th·ªÉ th√¨  

![](/img/user/assets/images/Pasted image 20250621224340.png)

C√≤n generate b·∫±ng model 2.5 - T·∫ßm T·∫ßm 2 Ti·∫øng. ƒê·ªÉ generate.


----
# Tu·∫ßn 10 15/8

- X·ª≠ d·ª•ng 2 embedding model: 
- 
D·ªØ li·ªáu 


Nh·∫≠n x√©t:
## ragas_results_20flash_gemi_emb2

| Metric                  | Average | Count <0.5 | Count ‚â•0.5 |
| ----------------------- | ------- | ---------- | ---------- |
| **faithfulness**        | 0.9450  | 8          | 280        |
| **answer_relevancy**    | 0.3763  | 125        | 163        |
| **context_recall**      | 0.4647  | 139        | 149        |
| **context_precision**   | 0.5069  | 142        | 146        |
| **semantic_similarity** | 0.8115  | 0          | 288        |
| **answer_correctness**  | 0.3805  | 211        | 77         |


H·∫°n ch·∫ø:
- H·∫°n ch·∫ø l√† L∆∞u tr·ªØ embedding c·ªßa c√°c ƒëi·ªÉm node -node v√† l∆∞u tr·ªØ c·ªßa c·∫£ embedding. C·∫ßn ram l∆∞u tr·ªØ ƒë·ªÉ load l√† t·ªõi 7gb (tr√™n colab - mang l√™n colab).
- Hi·ªán t·∫°i th√¨ em m·ªõi test vs d·ªØ li·ªáu c√¢u ƒë∆°n gi·∫£n ch·ªâ c√≥ 1 tr∆∞·ªùng. 
	- T√°ch ra 

Th·∫ßy n√≥i l
- N√™n vi·∫øt d·∫°ng paper
- C√°i th·ª© 2 th·ª≠ finetune tr√™n d·ªØ li·ªáu v√† test th·ª≠ - C√°i th·ª© 3 l√† test model deepseek v√† gwen. 


---
# 12/7/2025

|Tr∆∞·ªùng|Chunk c≈©|Chunk m·ªõi|TƒÉng/Gi·∫£m|% thay ƒë·ªïi|
|---|---|---|---|---|
|VLU|79|271|+192|**+243%**|
|TDTU|78|399|+321|**+411%**|
|NTTU|77|180|+103|**+134%**|
|HCMUE|74|354|+280|**+378%**|
|OU|65|231|+166|**+255%**|
|HCMUTE|59|188|+129|**+219%**|
|UMP|50|206|+156|**+312%**|
|UEH|47|242|+195|**+415%**|
|UIT|47|221|+174|**+370%**|
|BKU|43|216|+173|**+402%**|
|UFM|28|189|+161|**+575%**|
|HCMUS|27|149|+122|**+452%**|
|FTU2|24|107|+83|**+346%**|
|FPT|23|124|+101|**+439%**|
|PNTU|16|75|+59|**+369%**|
|**T·ªïng**|**737**|**3,152**|**+2,415**|**+327%**|

Saved combined data to: [E:\LLM_clone\KLTN\notebook\analyzeData\SemanticChunks\All_university.xlsx](file:///E:/LLM_clone/KLTN/notebook/analyzeData/SemanticChunks/All_university.xlsx)
**K·ªäCH B·∫¢N H·ªòI THO·∫†I CHATBOT TUY·ªÇN SINH (PH√ÇN THEO 9 INTENT)**

---

### üîπ INTENT 1: Th√¥ng tin chung v·ªÅ tr∆∞·ªùng

**M·ª•c ti√™u**: Gi·ªõi thi·ªáu c∆° b·∫£n v·ªÅ t√™n tr∆∞·ªùng, ƒë·ªãa ch·ªâ, s·ª© m·ªánh, website, li√™n h·ªá.

- Nh·∫≠n di·ªán: "UIT l√† g√¨?", "ƒê·ªãa ch·ªâ tr∆∞·ªùng HCMUS ·ªü ƒë√¢u v·∫≠y?", "Tr∆∞·ªùng n√†o thu·ªôc ƒêHQG?"
    
- Follow-up: "B·∫°n ƒëang quan t√¢m ƒë·∫øn tr∆∞·ªùng n√†o?"
    
- Ph·∫£n h·ªìi:
    
    - "Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin (UIT) tr·ª±c thu·ªôc ƒêHQG TP.HCM, n·∫±m t·∫°i Linh Trung, TP. Th·ªß ƒê·ª©c."
        
    - "B·∫°n c√≥ th·ªÉ truy c·∫≠p website ch√≠nh th·ª©c t·∫°i [https://uit.edu.vn](https://uit.edu.vn/) ho·∫∑c li√™n h·ªá qua s·ªë 028.372 52002."
        
- CTA: "B·∫°n c√≥ mu·ªën bi·∫øt th√™m ng√†nh ƒë√†o t·∫°o ho·∫∑c ch·ªâ ti√™u tuy·ªÉn sinh kh√¥ng?"
    
- Fallback: "M√¨nh ch∆∞a r√µ b·∫°n ƒëang h·ªèi v·ªÅ tr∆∞·ªùng n√†o, b·∫°n c√≥ th·ªÉ nh·∫≠p l·∫°i t√™n ƒë·∫ßy ƒë·ªß gi√∫p m√¨nh kh√¥ng?"
    

---

### üîπ INTENT 2: Ng√†nh ƒë√†o t·∫°o v√† ch∆∞∆°ng tr√¨nh h·ªçc

**M·ª•c ti√™u**: Gi·ªõi thi·ªáu ng√†nh h·ªçc, chuy√™n ng√†nh, m√¥ t·∫£ ng√†nh, th·ªùi gian ƒë√†o t·∫°o.

- Nh·∫≠n di·ªán: "UIT c√≥ ng√†nh AI kh√¥ng?", "H·ªçc C√¥ng ngh·ªá th·ª±c ph·∫©m ·ªü tr∆∞·ªùng n√†o?"
    
	- Follow-up: "B·∫°n mu·ªën bi·∫øt ng√†nh n√†y t·∫°i tr∆∞·ªùng n√†o? (VD: UIT, BKU, TDTU...)"
    
- Ph·∫£n h·ªìi:
    
    - "Ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o t·∫°i UIT l√† chuy√™n ng√†nh thu·ªôc C√¥ng ngh·ªá Th√¥ng tin, h·ªçc trong 4 nƒÉm v·ªõi n·ªôi dung g·ªìm h·ªçc m√°y, x·ª≠ l√Ω ng√¥n ng·ªØ, th·ªã gi√°c m√°y t√≠nh..."
        
    - "Ng√†nh C√¥ng ngh·ªá th·ª±c ph·∫©m ƒë∆∞·ª£c ƒë√†o t·∫°o t·∫°i TDTU, HCMUTE, v√† PNTU ‚Äì m·ªói tr∆∞·ªùng c√≥ ƒë·ªãnh h∆∞·ªõng kh√°c nhau (·ª©ng d·ª•ng, nghi√™n c·ª©u, th·ª±c h√†nh)"
        
- CTA: "B·∫°n mu·ªën xem chi ti·∫øt ch∆∞∆°ng tr√¨nh h·ªçc c·ªßa ng√†nh n√†y kh√¥ng?"
    
- Fallback: "B·∫°n vui l√≤ng cung c·∫•p th√™m ng√†nh h·ªçc v√† tr∆∞·ªùng c·ª• th·ªÉ nh√©!"
    

---

### üîπ INTENT 3: Ch·ªâ ti√™u tuy·ªÉn sinh

**M·ª•c ti√™u**: Th√¥ng tin s·ªë l∆∞·ª£ng ch·ªâ ti√™u, ng√†nh, h√¨nh th·ª©c tuy·ªÉn sinh.

- Nh·∫≠n di·ªán: "Ng√†nh An to√†n th√¥ng tin c√≥ bao nhi√™u ch·ªâ ti√™u?", "Tr∆∞·ªùng c√≥ x√©t h·ªçc b·∫° kh√¥ng?"
    
- Follow-up: "B·∫°n h·ªèi ng√†nh n√†o v√† tr∆∞·ªùng n√†o nh√©?"
    
- Ph·∫£n h·ªìi:
    
    - "Ng√†nh An to√†n th√¥ng tin t·∫°i BKU c√≥ kho·∫£ng 300 ch·ªâ ti√™u."
        
    - "Tr∆∞·ªùng c√≥ x√©t tuy·ªÉn h·ªçc b·∫°, ƒëi·ªÉm thi THPT v√† ƒëi·ªÉm ƒê√°nh gi√° nƒÉng l·ª±c tu·ª≥ t·ª´ng ng√†nh."
        
- CTA: "B·∫°n mu·ªën m√¨nh g·ª≠i b·∫°n link x√©t tuy·ªÉn c·ªßa tr∆∞·ªùng kh√¥ng?"
    
- Fallback: "Th√¥ng tin b·∫°n cung c·∫•p ch∆∞a ƒë·ªß, b·∫°n b·ªï sung t√™n ng√†nh v√† tr∆∞·ªùng nh√©."
    

---

### üîπ INTENT 4: ƒêi·ªÅu ki·ªán nh·∫≠p h·ªçc v√† h·ªçc ph√≠

**M·ª•c ti√™u**: Cung c·∫•p ƒëi·ªÅu ki·ªán ƒë·∫ßu v√†o, gi·∫•y t·ªù, m·ª©c h·ªçc ph√≠, h·ªçc b·ªïng.

- Nh·∫≠n di·ªán: "ƒêi·ªÅu ki·ªán v√†o ng√†nh Y l√† g√¨?", "H·ªçc ph√≠ ng√†nh CNTT l√† bao nhi√™u?"
    
- Follow-up: "B·∫°n h·ªçc ch∆∞∆°ng tr√¨nh ƒë·∫°i tr√† hay ch·∫•t l∆∞·ª£ng cao? ·ªû tr∆∞·ªùng n√†o nh√©?"
    
- Ph·∫£n h·ªìi:
    
    - "Ng√†nh Y ƒëa khoa t·∫°i UMP y√™u c·∫ßu ƒëi·ªÉm chu·∫©n cao, h·ªì s∆° c·∫ßn CMND, h·ªçc b·∫°, gi·∫•y x√°c nh·∫≠n ∆∞u ti√™n..."
        
    - "H·ªçc ph√≠ ng√†nh CNTT t·∫°i UIT kho·∫£ng 12-15 tri·ªáu/nƒÉm (ƒë·∫°i tr√†). Ch·∫•t l∆∞·ª£ng cao t·ª´ 30 tri·ªáu/nƒÉm."
        
- CTA: "B·∫°n mu·ªën xem th√™m c√°c lo·∫°i h·ªçc b·ªïng hi·ªán h√†nh kh√¥ng?"
    
- Fallback: "Ch∆∞a r√µ ng√†nh/tr∆∞·ªùng n√™n ch∆∞a b√°o ch√≠nh x√°c, b·∫°n cho m√¨nh bi·∫øt r√µ h∆°n nh√©."
    

---

### üîπ INTENT 5: L·ªãch tr√¨nh tuy·ªÉn sinh

**M·ª•c ti√™u**: Cung c·∫•p c√°c m·ªëc th·ªùi gian nh·∫≠n h·ªì s∆°, thi, nh·∫≠p h·ªçc...

- Nh·∫≠n di·ªán: "Khi n√†o n·ªôp h·ªì s∆° v·∫≠y?", "Bao gi·ªù c√≥ k·∫øt qu·∫£?"
    
- Follow-up: "B·∫°n x√©t tuy·ªÉn theo ph∆∞∆°ng th·ª©c n√†o v√† t·∫°i tr∆∞·ªùng n√†o nh√©?"
    
- Ph·∫£n h·ªìi:
    
    - "X√©t tuy·ªÉn h·ªçc b·∫° t·∫°i TDTU b·∫Øt ƒë·∫ßu t·ª´ 10/6 ƒë·∫øn 15/7. K·∫øt qu·∫£ d·ª± ki·∫øn c√¥ng b·ªë 22/7."
        
    - "N·∫øu b·∫°n thi ƒêGNL th√¨ m·ªëc tuy·ªÉn sinh c√≥ th·ªÉ kh√°c, m√¨nh c√≥ th·ªÉ g·ª≠i b·∫°n link chi ti·∫øt."
        
- CTA: "B·∫°n mu·ªën chatbot nh·∫Øc l·ªãch cho b·∫°n qua email ho·∫∑c Zalo kh√¥ng?"
    
- Fallback: "B·∫°n vui l√≤ng cho bi·∫øt th√™m ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn ho·∫∑c t√™n tr∆∞·ªùng c·ª• th·ªÉ."
    

---

### üîπ INTENT 6: Ch√≠nh s√°ch h·ªó tr·ª£ sinh vi√™n

**M·ª•c ti√™u**: Gi·ªõi thi·ªáu c√°c ch√≠nh s√°ch vay v·ªën, h·ªçc t·∫≠p, vi·ªác l√†m sau t·ªët nghi·ªáp.

- Nh·∫≠n di·ªán: "C√≥ ch√≠nh s√°ch vay v·ªën kh√¥ng?", "Ra tr∆∞·ªùng d·ªÖ xin vi·ªác kh√¥ng?"
    
- Follow-up: "B·∫°n mu·ªën bi·∫øt v·ªÅ ch√≠nh s√°ch t·∫°i tr∆∞·ªùng n√†o v√† ng√†nh n√†o?"
    
- Ph·∫£n h·ªìi:
    
    - "TDTU c√≥ ch√≠nh s√°ch vay v·ªën h·ªçc t·∫≠p qua ng√¢n h√†ng ch√≠nh s√°ch."
        
    - "Sinh vi√™n ng√†nh CNTT t·∫°i UIT c√≥ t·ª∑ l·ªá c√≥ vi·ªác sau 6 th√°ng ra tr∆∞·ªùng l√™n ƒë·∫øn 95%."
        
- CTA: "B·∫°n c√≥ mu·ªën bi·∫øt th√™m v·ªÅ c√°c ch∆∞∆°ng tr√¨nh li√™n k·∫øt doanh nghi·ªáp?"
    
- Fallback: "Ch∆∞a r√µ ng√†nh ho·∫∑c tr∆∞·ªùng, b·∫°n cung c·∫•p th√™m nh√©."
    

---

### üîπ INTENT 7: C∆° s·ªü v·∫≠t ch·∫•t

**M·ª•c ti√™u**: Cung c·∫•p th√¥ng tin v·ªÅ th∆∞ vi·ªán, KTX, ph√≤ng lab, c∆° s·ªü ƒë√†o t·∫°o.

- Nh·∫≠n di·ªán: "Tr∆∞·ªùng c√≥ KTX kh√¥ng?", "Ph√≤ng lab c√≥ hi·ªán ƒë·∫°i kh√¥ng?"
    
- Follow-up: "B·∫°n ƒëang h·ªèi tr∆∞·ªùng n√†o v·∫≠y?"
    
- Ph·∫£n h·ªìi:
    
    - "UIT c√≥ 2 khu KTX l·ªõn v√† ph√≤ng m√°y ƒë∆∞·ª£c trang b·ªã GPU ph·ª•c v·ª• AI."
        
    - "HCMUS c√≥ th∆∞ vi·ªán hi·ªán ƒë·∫°i m·ªü c·ª≠a 7h‚Äì21h, r·∫•t ph√π h·ª£p cho sinh vi√™n nghi√™n c·ª©u."
        
- CTA: "B·∫°n mu·ªën m√¨nh g·ª≠i b·∫°n b·∫£n ƒë·ªì c∆° s·ªü v·∫≠t ch·∫•t kh√¥ng?"
    
- Fallback: "M√¨nh ch∆∞a r√µ b·∫°n h·ªèi tr∆∞·ªùng n√†o, c√≥ th·ªÉ nh·∫≠p l·∫°i kh√¥ng?"
    

---

### üîπ INTENT 8: Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ƒë·∫∑c bi·ªát

**M·ª•c ti√™u**: Li√™n k·∫øt qu·ªëc t·∫ø, th·∫°c sƒ©, h·ªçc song b·∫±ng.

- Nh·∫≠n di·ªán: "Tr∆∞·ªùng c√≥ ch∆∞∆°ng tr√¨nh du h·ªçc kh√¥ng?", "C√≥ h·ªçc th·∫°c sƒ© CNTT kh√¥ng?"
    
- Follow-up: "B·∫°n mu·ªën h·ªçc ch∆∞∆°ng tr√¨nh n√†o ‚Äì li√™n k·∫øt, du h·ªçc hay h·ªçc sau ƒë·∫°i h·ªçc?"
    
- Ph·∫£n h·ªìi:
    
    - "OU c√≥ li√™n k·∫øt v·ªõi c√°c tr∆∞·ªùng t·∫°i Canada v√† Nh·∫≠t ‚Äì c√≥ ch∆∞∆°ng tr√¨nh 2+2."
        
    - "UIT c√≥ ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o th·∫°c sƒ© CNTT h·ªá chu·∫©n v√† h·ªá ƒë·ªãnh h∆∞·ªõng ·ª©ng d·ª•ng."
        
- CTA: "B·∫°n mu·ªën xem ƒëi·ªÅu ki·ªán tham gia ch∆∞∆°ng tr√¨nh kh√¥ng?"
    
- Fallback: "B·∫°n ch∆∞a n√≥i r√µ lo·∫°i ch∆∞∆°ng tr√¨nh b·∫°n quan t√¢m."
    

---

### üîπ INTENT 9: So s√°nh tr∆∞·ªùng theo m·ª•c ti√™u c√° nh√¢n

**M·ª•c ti√™u**: H∆∞·ªõng ng∆∞·ªùi h·ªçc ch·ªçn ƒë√∫ng tr∆∞·ªùng/ng√†nh ph√π h·ª£p ƒë·ªãnh h∆∞·ªõng c√° nh√¢n.

- Nh·∫≠n di·ªán: "N√™n h·ªçc AI ·ªü ƒë√¢u t·ªët gi·ªØa BKU, UIT v√† HCMUS?"
    
- Follow-up: "B·∫°n ∆∞u ti√™n ƒëi·ªÅu g√¨: ch∆∞∆°ng tr√¨nh h·ªçc, c∆° h·ªôi vi·ªác l√†m hay nghi√™n c·ª©u?"
    
- Ph·∫£n h·ªìi:
    
    - "UIT m·∫°nh v·ªÅ AI th·ª±c h√†nh, c√≥ nhi·ªÅu lab h·ªó tr·ª£.  
        BKU c√≥ th∆∞∆°ng hi·ªáu v√† h·ª£p t√°c doanh nghi·ªáp t·ªët.  
        HCMUS thi√™n v·ªÅ nghi√™n c·ª©u, h·ªçc thu·∫≠t."
        
    - "N·∫øu b·∫°n mu·ªën h·ªçc CNTT nh∆∞ng c·∫ßn th·ªùi gian t·ª± h·ªçc th√™m k·ªπ nƒÉng kh√°c, FPT v√† UIT l√† l·ª±a ch·ªçn t·ªët h∆°n BKU."
        
- CTA: "B·∫°n mu·ªën xem b·∫£ng so s√°nh chi ti·∫øt kh√¥ng?"
    
- Fallback: "B·∫°n ch∆∞a cung c·∫•p ƒë·ªß th√¥ng tin m·ª•c ti√™u ngh·ªÅ nghi·ªáp ‚Äì b·∫°n b·ªï sung nh√©."
    

---
![](/img/user/assets/images/Pasted image 20250713164414.png)


# 23/5/2025
- Embedding model c·∫ßn t·∫°o ra ho·∫∑c finetune model ƒë·ªÉ ra k·∫øt qu·∫£ nh∆∞ th·∫ø n√†o. 
- GIao di·ªán code
	- C√≥ ch·ªó explain k·∫øt qu·∫£ - hightlight l·∫°i ƒëi·ªÉm ch√≠nh
		- Lu·ªìng l√† khi generate xong th√¨ ƒë√£ l∆∞u database. Th√¨ c√≥ get v·ªÅ r·ªìi m·ªõi qua model explain ƒë·ªÉ hi·ªÉn th·ªã 
		- R·ªìi c√≥ hi·ªán th·ªã k·∫øt qu·∫£ recall, hay ƒë√°nh gi√° g√¨ c≈©ng ch·ªó n√†y. 
	- Gi·∫£i th√≠ch retrieval 
		- C√≤n test th√¨ ph·∫£i ch·ªß y·∫øu d√πng embedding ƒë·ªÉ test. 
	- Visualize ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi - Graph rag v√† (graph rag qdrant (ch∆∞a th·ª≠)
		- C√≤n langgraph th√¨ chia ra l√†m c√°c ƒëi·ªÉm node ƒë·ªÉ g·∫Øn log ƒë·ªÉ ch·∫°y c√°c ƒëi·ªÉm time c·ªßa t·ª´ng c√°i. 
	- Nh·∫•n m·∫°nh embedding model (c·∫ßn g√¨ ƒë√£ train d·ªØ li·ªáu g√¨ ) - t·∫°o ra k·∫øt qu·∫£ benchmark c√≥ v√† kh√¥ng ƒë·ªÉ ch·ª©ng m√¨nh kh·∫£ nƒÉng retrieval. *** **important**


# 9/8

https://medium.com/@rajveer.rathod1301/evaluating-llm-responses-with-deepeval-library-a-comprehensive-practical-guide-e55ef1f9eeab


# 16/8
C√ín nh·ªØng g√¨
- VI·∫øt b√°o c√°o
- embedding:
	- T√ånh n·ªët embedding c·ªßa c√¢u h·ªèi kh√≥ complex: T√çnh ra c√¢u ground truth ƒë√£ - xong r - m·ªõi t√≠nh - K·∫øt qu·∫£ retrieval - kq - generate. 
- S·ª≠a code: 
	- NOde ƒë·ªông - v√† c√≥ ƒë∆∞·ªùng ƒëi
- CHain of thoughj
- Viualize
- enhance test embedding -
- mAYTROLASTOPKI. 
- v·∫≠y l√† ch·∫°y 30 epoch

HCMUE, FTU2, HCMUS, UIT , VLU
https://chatgpt.com/c/68a9c739-664c-8326-b1f3-b504911c1330?model=gpt-5-thinking
# 20/8
### 3.5.2 Limitations and Challenges

**Node construction complexity.** Transforming raw documents into graph nodes is inherently non-trivial. Our design chooses to derive nodes from tags and topics rather than from named entities. This choice simplifies upstream NLP but introduces granularity and consistency issues. Tags and topics can be long, multiword phrases with variable specificity, which leads to heterogeneous node definitions across documents. Inconsistent granularity inflates the node set, increases redundancy, and makes downstream edge calibration more sensitive to threshold choices.

**Label length and information loss.** Because tag and topic strings are often long, we currently display only the first tag as the node label. This improves readability but discards potentially critical qualifiers present in subsequent tags. The compromise can produce label collisions and reduce the discriminative power of the visualization. A principled solution is to separate internal node identifiers from human-readable labels, compress labels with keyphrase summarization, and expose the full tag list via hover tooltips or detail panels.

**Edge definition and weighting.** Edges are created from semantic proximity between node representations, then weighted by similarity scores. This process is sensitive to multiple hyperparameters: chunk size, embedding model, similarity metric, and threshold or neighborhood size. Overly permissive thresholds create ‚Äúhairball‚Äù graphs with hub nodes, while conservative thresholds fragment the graph and degrade multi-hop retrieval. A more robust recipe is to start from a k-nearest-neighbor backbone, enforce mutual-k constraints, and re-weight edges with local rank normalization. Community detection can be used to prune cross-community edges that contribute little to retrieval quality.

**Scalability and maintenance.** Pairwise similarity for graph construction scales quadratically with the number of chunks. Incremental updates introduce further complexity since newly added chunks must be embedded and linked consistently with the existing topology. Practical operation requires approximate nearest-neighbor indices for candidate edge discovery, batchable graph updates, versioned graph snapshots, and background compaction that merges near-duplicate nodes.

**Static visualization limitations.** The current implementation relies on NetworkX to render static images. Static layouts do not convey traversal dynamics, cause overlapping nodes and edge crossings, and limit user comprehension of the reasoning path. Without interactive pan, zoom, filtering, and path highlighting, the explanatory value is reduced. This limitation is amplified by long tag and topic labels that compete for screen real estate. Switching to an interactive renderer with force-directed or multilevel layouts, edge bundling, label elision with tooltips, and explicit path highlighting materially improves transparency and usability.

**Blending tags, topics, and text distances.** Our similarity graph blends tag and topic features with passage text embeddings to define neighborhood structure. While this captures topical proximity, it can also bias the graph toward high-frequency tags and underrepresent rare but salient connections. A better compromise is a multi-layer representation that separates a document layer from a tag-topic layer, connects layers with typed edges, and performs retrieval on the document layer while using the tag-topic layer for reranking and explanations.

**Explainability and provenance.** The current pipeline does not attach rich provenance to edges and paths. Users see which nodes were traversed but not why specific edges were preferred. Storing per-edge evidence such as shared keyphrases, similarity decompositions, and source spans, then surfacing that evidence in the visualizer, strengthens trust and makes failure analysis tractable.

**Evaluation difficulty.** Conventional IR metrics quantify ranking quality but do not fully capture graph-assisted multi-hop reasoning. Path-aware evaluation remains an open challenge. In practice, we combine standard top-k metrics with qualitative audits of reasoning paths and small-scale user studies to validate whether graph traversal adds value beyond a strong vector-only baseline.

**Resource constraints.** Long labels, large node counts, and dense neighborhoods stress memory and rendering performance. On the retrieval side, graph-aware query engines add latency compared with pure ANN search. Careful path pruning, small beam sizes, and staged retrieval with a vector prefilter followed by graph reranking keep latency within interactive budgets.

**Domain shift and cold start.** Tag and topic vocabularies are corpus-dependent. When new institutions or academic subdomains are incorporated, node definitions and edge weights can drift, degrading retrieval until the graph is refreshed. Scheduled re-indexing and lightweight online calibration mitigate these effects.

**Design choices specific to this work.** Using tags and topics instead of named entities simplifies extraction and increases coverage across varied document styles. The trade-off is noisier node definitions, long labels, and reduced controllability of node semantics. Static NetworkX visualization is quick to prototype but limits insight into reasoning paths and increases label overlap. To partially compensate, we currently display only the first tag as the node label and rely on blended tag-topic plus passage embeddings to encode inter-passage distances. This is a pragmatic compromise but should be considered an interim solution.

**Mitigation roadmap.** Short-term improvements include hierarchical chunking to stabilize granularity, k-NN plus mutual-k backbones with percentile-based edge thresholds, interactive visualization with label elision and tooltips, and explicit path highlighting with edge scores. Medium-term steps include adopting a two-layer graph with typed edges, storing provenance for each edge, and learning edge weights from held-out QA signals. Long-term, a graph coarsening pipeline with community-level nodes and on-demand refinement balances interpretability, retrieval quality, and performance at scale.